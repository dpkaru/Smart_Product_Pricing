{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0101c489-9332-4a64-8c54-eb4c8d4c560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20bde75-878b-4df4-80b3-d3421576dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\dhruv\\AI for future course\\data\")\n",
    "#load the data\n",
    "df=pd.read_csv(r'C:\\Users\\dhruv\\ML_dataset\\student_resource\\dataset\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba860f0-75dd-4a7b-acb9-19c9d6c91c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id            int64\n",
       "catalog_content     object\n",
       "image_link          object\n",
       "price              float64\n",
       "Item Name           object\n",
       "Value              float64\n",
       "Unit                object\n",
       "Bullet Point 1      object\n",
       "Bullet Point 2      object\n",
       "Bullet Point 3      object\n",
       "Bullet Point 4      object\n",
       "Bullet Point 5      object\n",
       "Value_numeric      float64\n",
       "combined_text       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc362f9-6ad1-4b0a-baef-f6faf3778c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 14)\n",
      "TF-IDF: max_features=20000, SVD=80\n",
      "TF-IDF shape: (150000, 20000)\n",
      "SVD shape: (150000, 80)\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20400\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 23.598634\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 14.6617\tvalid_0's l2: 1179.41\n",
      "[400]\tvalid_0's l1: 14.4525\tvalid_0's l2: 1168.3\n",
      "[600]\tvalid_0's l1: 14.4109\tvalid_0's l2: 1166.28\n",
      "[800]\tvalid_0's l1: 14.3482\tvalid_0's l2: 1164.34\n",
      "Early stopping, best iteration is:\n",
      "[759]\tvalid_0's l1: 14.3576\tvalid_0's l2: 1164.05\n",
      "\n",
      "ðŸ”¹ Validation SMAPE: 64.15%\n",
      "\n",
      "âœ… Predictions saved to test_out.csv\n",
      "âœ… Final Validation SMAPE = 64.15%\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# SMART PRODUCT PRICING PREDICTION\n",
    "# Goal: SMAPE < 40\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# =====================================================\n",
    "# Step 1: Load Data\n",
    "# =====================================================\n",
    "train = pd.read_csv(r'C:\\Users\\dhruv\\ML_dataset\\student_resource\\dataset\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\dhruv\\ML_dataset\\student_resource\\dataset\\test.csv')\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# =====================================================\n",
    "# Step 2: Preprocess Text\n",
    "# =====================================================\n",
    "train[\"catalog_content\"] = train[\"catalog_content\"].fillna(\"\")\n",
    "test[\"catalog_content\"] = test[\"catalog_content\"].fillna(\"\")\n",
    "\n",
    "# Combine text for unified vectorization\n",
    "all_text = pd.concat([train[\"catalog_content\"], test[\"catalog_content\"]])\n",
    "\n",
    "# =====================================================\n",
    "# Step 3: Dynamic TF-IDF based on RAM\n",
    "# =====================================================\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024 ** 3)\n",
    "if available_memory_gb > 8:\n",
    "    max_features = 80000\n",
    "    n_components = 250\n",
    "elif available_memory_gb > 4:\n",
    "    max_features = 40000\n",
    "    n_components = 150\n",
    "else:\n",
    "    max_features = 20000\n",
    "    n_components = 80\n",
    "\n",
    "print(f\"TF-IDF: max_features={max_features}, SVD={n_components}\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    stop_words=\"english\",\n",
    "    sublinear_tf=True\n",
    ")\n",
    "tfidf_all = tfidf.fit_transform(all_text)\n",
    "print(\"TF-IDF shape:\", tfidf_all.shape)\n",
    "\n",
    "# =====================================================\n",
    "# Step 4: Dimensionality Reduction (Memory Safe)\n",
    "# =====================================================\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "svd_all = svd.fit_transform(tfidf_all)\n",
    "print(\"SVD shape:\", svd_all.shape)\n",
    "\n",
    "# Split back\n",
    "train_svd = svd_all[:len(train)]\n",
    "test_svd = svd_all[len(train):]\n",
    "\n",
    "# Clean up to free memory\n",
    "del tfidf_all, svd_all, all_text\n",
    "gc.collect()\n",
    "\n",
    "# =====================================================\n",
    "# Step 5: Train/Test Split\n",
    "# =====================================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_svd, train[\"price\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Step 6: Define SMAPE Metric\n",
    "# =====================================================\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(\n",
    "        np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n",
    "    ) * 100\n",
    "\n",
    "# =====================================================\n",
    "# Step 7: Train LightGBM Model\n",
    "# =====================================================\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=128,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='mae',\n",
    "    callbacks=[early_stopping(100), log_evaluation(200)]\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Step 8: Evaluate on Validation\n",
    "# =====================================================\n",
    "y_pred_val = model.predict(X_val)\n",
    "val_smape = smape(y_val, y_pred_val)\n",
    "print(f\"\\nðŸ”¹ Validation SMAPE: {val_smape:.2f}%\")\n",
    "\n",
    "# =====================================================\n",
    "# Step 9: Predict on Test and Export\n",
    "# =====================================================\n",
    "test_preds = model.predict(test_svd)\n",
    "test_preds = np.maximum(0, test_preds)  # Ensure positive prices\n",
    "\n",
    "output = pd.DataFrame({\n",
    "    \"sample_id\": test[\"sample_id\"],\n",
    "    \"price\": test_preds\n",
    "})\n",
    "\n",
    "output.to_csv(\"test_out.csv\", index=False)\n",
    "print(\"\\nâœ… Predictions saved to test_out.csv\")\n",
    "print(f\"âœ… Final Validation SMAPE = {val_smape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81102e46-502c-4b2d-a86b-9149b8566e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3bf14e-8114-41eb-8fe2-dafa66c1b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.598634\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.558059\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.643564\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.673561\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.603754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 48000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 23.514229\n",
      "\n",
      "===== Validation Results =====\n",
      "SMAPE: 71.68\n",
      "MAE: 16.59\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: Item Name: object, Unit: object, Bullet Point 1: object, Bullet Point 2: object, Bullet Point 3: object, Bullet Point 4: object, Bullet Point 5: object, combined_text: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 132\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Step 9: Predict on Test\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mpredict(test)\n\u001b[0;32m    133\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted_Price\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_pred})\n\u001b[0;32m    134\u001b[0m submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:379\u001b[0m, in \u001b[0;36m_BaseStacking.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    Predicted targets.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    378\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_estimator_\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:984\u001b[0m, in \u001b[0;36mStackingRegressor.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    971\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the predictions for X for each estimator.\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:304\u001b[0m, in \u001b[0;36m_BaseStacking._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    303\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(est, meth)(X)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m ]\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\sklearn.py:1144\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m _choose_param_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m, predict_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m   1142\u001b[0m predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_n_jobs(predict_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mpredict(  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     X,\n\u001b[0;32m   1146\u001b[0m     raw_score\u001b[38;5;241m=\u001b[39mraw_score,\n\u001b[0;32m   1147\u001b[0m     start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   1148\u001b[0m     num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   1149\u001b[0m     pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf,\n\u001b[0;32m   1150\u001b[0m     pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib,\n\u001b[0;32m   1151\u001b[0m     validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params,\n\u001b[0;32m   1153\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:4767\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   4765\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4766\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   4768\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   4769\u001b[0m     start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   4770\u001b[0m     num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   4771\u001b[0m     raw_score\u001b[38;5;241m=\u001b[39mraw_score,\n\u001b[0;32m   4772\u001b[0m     pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf,\n\u001b[0;32m   4773\u001b[0m     pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib,\n\u001b[0;32m   4774\u001b[0m     data_has_header\u001b[38;5;241m=\u001b[39mdata_has_header,\n\u001b[0;32m   4775\u001b[0m     validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   4776\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:1158\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     _safe_call(\n\u001b[0;32m   1150\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterValidateFeatureNames(\n\u001b[0;32m   1151\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m         )\n\u001b[0;32m   1155\u001b[0m     )\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m-> 1158\u001b[0m     data \u001b[38;5;241m=\u001b[39m _data_from_pandas(\n\u001b[0;32m   1159\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   1160\u001b[0m         feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m         categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1162\u001b[0m         pandas_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical,\n\u001b[0;32m   1163\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1165\u001b[0m predict_type \u001b[38;5;241m=\u001b[39m _C_API_PREDICT_NORMAL\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_score:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:868\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdf_dtypes)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 868\u001b[0m     _pandas_to_numpy(data, target_dtype\u001b[38;5;241m=\u001b[39mtarget_dtype),\n\u001b[0;32m    869\u001b[0m     feature_name,\n\u001b[0;32m    870\u001b[0m     categorical_feature,\n\u001b[0;32m    871\u001b[0m     pandas_categorical,\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:814\u001b[0m, in \u001b[0;36m_pandas_to_numpy\u001b[1;34m(data, target_dtype)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pandas_to_numpy\u001b[39m(\n\u001b[0;32m    811\u001b[0m     data: pd_DataFrame,\n\u001b[0;32m    812\u001b[0m     target_dtype: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.typing.DTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 814\u001b[0m     _check_for_bad_pandas_dtypes(data\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:805\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[1;34m(pandas_dtypes_series)\u001b[0m\n\u001b[0;32m    799\u001b[0m bad_pandas_dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    803\u001b[0m ]\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: Item Name: object, Unit: object, Bullet Point 1: object, Bullet Point 2: object, Bullet Point 3: object, Bullet Point 4: object, Bullet Point 5: object, combined_text: object"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Smart Product Pricing Model - Optimized Ensemble\n",
    "# Target SMAPE < 40\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# =====================================================\n",
    "# Step 1: Load Data\n",
    "# =====================================================\n",
    "train = pd.read_csv(r'C:\\Users\\dhruv\\ML_dataset\\student_resource\\dataset\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\dhruv\\ML_dataset\\student_resource\\dataset\\test.csv')\n",
    "\n",
    "# Replace with your actual target column\n",
    "TARGET = \"price\" if \"price\" in train.columns else train.columns[-1]\n",
    "\n",
    "# =====================================================\n",
    "# Step 2: Basic Cleaning\n",
    "# =====================================================\n",
    "train = train.dropna(subset=[TARGET])\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "# =====================================================\n",
    "# Step 3: Feature Engineering\n",
    "# =====================================================\n",
    "# Convert categorical variables to numeric\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str)) + list(test[col].astype(str)))\n",
    "        train[col] = le.transform(train[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "# Add polynomial features\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c != TARGET]\n",
    "for col in numeric_cols:\n",
    "    train[col + \"_sq\"] = train[col] ** 2\n",
    "    test[col + \"_sq\"] = test[col] ** 2\n",
    "    train[col + \"_log\"] = np.log1p(train[col])\n",
    "    test[col + \"_log\"] = np.log1p(test[col])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "train[numeric_cols] = scaler.fit_transform(train[numeric_cols])\n",
    "test[numeric_cols] = scaler.transform(test[numeric_cols])\n",
    "\n",
    "# =====================================================\n",
    "# Step 4: Split Data\n",
    "# =====================================================\n",
    "X = train.drop(columns=[TARGET])\n",
    "y = train[TARGET]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =====================================================\n",
    "# Step 5: Define Models\n",
    "# =====================================================\n",
    "lgb = LGBMRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=10,\n",
    "    num_leaves=60,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cat = CatBoostRegressor(\n",
    "    iterations=1200,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function='MAE',\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=1200,\n",
    "    learning_rate=0.04,\n",
    "    max_depth=9,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Step 6: Stacking Ensemble\n",
    "# =====================================================\n",
    "ensemble = StackingRegressor(\n",
    "    estimators=[('lgb', lgb), ('cat', cat), ('xgb', xgb)],\n",
    "    final_estimator=Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Step 7: Train Model\n",
    "# =====================================================\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# =====================================================\n",
    "# Step 8: Evaluate Model\n",
    "# =====================================================\n",
    "y_pred_val = ensemble.predict(X_val)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-9))\n",
    "\n",
    "smape_val = smape(y_val, y_pred_val)\n",
    "mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "\n",
    "print(\"\\n===== Validation Results =====\")\n",
    "print(f\"SMAPE: {smape_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "# =====================================================\n",
    "# Step 9: Predict on Test\n",
    "# =====================================================\n",
    "test_pred = ensemble.predict(test)\n",
    "submission = pd.DataFrame({\"Predicted_Price\": test_pred})\n",
    "submission.to_csv(\"final_submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to final_submission.csv âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3608d58f-3e72-482d-9818-918013c30bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (4.57.0)\n",
      "Requirement already satisfied: timm in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (1.0.20)\n",
      "Requirement already satisfied: torch in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (0.23.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from lightgbm) (1.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers timm torch torchvision lightgbm numpy pandas scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9b536-af9c-42c2-b342-1771da756649",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets \"huggingface_hub[hf_xet]\" --quiet\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb76562-2194-4c83-8442-a54e9d591aa7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
